---
title: "Calorie Burning Report"
output: pdf_document
---

# Methods and procedure

## Data exploration
First let's load the data and look at the summary.
```{r}
requirements = c("nlme", "effects", "pastecs", "lattice", "psych", "ggplot2", "GGally", "mice", "VIM", "aod", "BaM", "lme4")
if (length(setdiff(requirements, rownames(installed.packages()))) > 0) {
  invisible(install.packages(setdiff(requirements, rownames(installed.packages())))) 
}
for (i in seq(1, length(requirements))) {
  invisible(library(requirements[i], character.only=T))
}
options(digits=4)
muscledata = read.table("muscle-incomplete.txt", header=T, na.strings = "NA")
summary(muscledata)
```

Here are some descriptive statistics.
```{r}
plot(muscledata)
```

Let's check correlation between the weight of an individual and the intensity of their workout, as defined by the calories burnt per hour, using the Pearson correlation coefficient:
```{r}
cor.test(muscledata$weight, muscledata$calhour, alternative="two.sided", method="pearson")
```

The P value is way, way more than 0.05. We accept the null hypothesis, there seems to be no significant correlation. The intesities of workout seem to be randomly distributed among people of different weights, which is optimal to draw conclusions concerning heat production.

Let's try plotting a simple linear model to this relationship.
```{r}
weightCalhr = lm(weight~calhour, data=muscledata)
summary(weightCalhr)
```
The linear model confirms our suspicion that there is no relationship here.

Let's try to explain heat production in function of weight and intensity of the workout, whilst allowing for interaction of the 2 predictors (whilst increasing intensity of workout, a higher weight could result in a different speed of heat production increase):
```{r}
muscledata_edit = na.omit(muscledata)
muscledata.complete.case = lm(calories~weight+calhour+weight*calhour, data=muscledata_edit)
muscledata.complete.case.summary = summary(muscledata.complete.case)
muscledata.complete.case.summary
plot(allEffects(muscledata.complete.case))
```

Using the summary method, we conclude that adding weight, calhour and interaction to a model that already has the other possible components results in a significant increase in explanatory power. (note to group: was explained in last 10 slides of chapter 1, he'll probably ask about this if we don't mention it since using the anova method reults in a different interpretation).



```{r}
plot(allEffects(muscledata.complete.case))
```

There seems to be a relationship between calhour and missing data. Clearly, there is a Missing at Random (MAR) mechanism involved - P(Missing) depends only on the observed values of calhour.

```{r}
muscledata.complete.case.2 = lm(calories~calhour+weight+calhour*weight, data=muscledata_edit)
summary(muscledata.complete.case.2)
plot(allEffects(muscledata.complete.case.2))
```


## Missing data exploration

```{r}
aggr(muscledata, numbers = TRUE, prop = FALSE, ylab = c("Histogram of missing data", "Pattern"))
```


```{r}
aggr(muscledata, combined=TRUE, numbers = TRUE, prop = TRUE, cex.numbers=0.87, varheight = FALSE)
barMiss(muscledata[,c("calories","weight")])
histMiss(muscledata)
histMiss(muscledata, pos=2)
```



## Complete case analysis

## Multiple imputation analysis

```{r}
invisible(capture.output(muscledata.imp <- mice(muscledata, meth = c("", "", "norm"), m=100))) # imputation of different values, 100 different complete datasets
invisible(muscledata.fit <- with(data=muscledata.imp, exp=glm(calories~weight+calhour+weight*calhour))) # analysis, creating a Q for each imputed dataset
muscledata.est <- pool(muscledata.fit) # pooling the Qs together to create one estimate Q mean. if Q are approx normally distributed, we calculate mean over all Q and sum the within and between imputation variance using Rubins method
summary(muscledata.est)
```

```{r}
MI.fitted.values = complete(muscledata.imp, "long", inc=T)
muscledata.results.MIALL <- glm(calories~weight+calhour+weight*calhour, data=MI.fitted.values)
dlist=list(calhour=seq(20,60,10))
plot(allEffects(muscledata.results.MIALL,xlevels=dlist)[1])
```

Clearly something cool happened here.

```{r}
col <- rep(c("pink","purple")[1+as.numeric(is.na(muscledata.imp$data$calories))],101)
stripplot(calories~.imp, data=MI.fitted.values, jit=TRUE, fac=0.8, col=col, pch=20, cex=1.4, xlab="Imputation number")
```

```{r}
muscledata = read.table("muscle-incomplete.txt", header=T, na.strings = "NA")
muscledata.without13 = muscledata[!muscledata$calhour == 13,]
invisible(capture.output(muscledata.without13.imp <- mice(muscledata.without13, meth = c("", "", "norm"), m=100))) # imputation of different values, 100 different complete datasets
invisible(muscledata.without13.fit <- with(data=muscledata.without13.imp, exp=glm(calories~weight+calhour+weight*calhour))) # analysis, creating a Q for each imputed dataset
muscledata.without13.est <- pool(muscledata.without13.fit) # pooling the Qs together to create one estimate Q mean. if Q are approx normally distributed, we calculate mean over all Q and sum the within and between imputation variance using Rubins method
summary(muscledata.without13.est)
```

## IPW analysis

```{r}
muscledata$r<-as.numeric(!is.na(muscledata$calories))
head(muscledata)
muscledata.ipw.glm<-glm(r ~ weight+calhour+weight*calhour, data=muscledata,family=binomial)
summary(muscledata.ipw.glm)
```

```{r}
muscledata$w<-1/fitted(muscledata.ipw.glm)
muscledata.results.ipw<- glm(calories~weight+calhour+weight*calhour, data=muscledata, weights=muscledata$w)
summary(muscledata.results.ipw)
plot(allEffects(muscledata.results.ipw))
```



# Discussion

And in my opnion, Bayesian would make sense. MI is better in here. You cannot give weights to lost data. 

# Conclusion

The missing data is correlated with the calhour - intensity of the exercise - hence there is something wrong with the experimental design. Such as the way they measured heat production, so they could not accurately measure calorie burning. While we have no data for low calhour values, attributing weights to the values we have is not workable for the 13 calhour data point. That being said, the MI approach provides a more robust estimates for missing data.

