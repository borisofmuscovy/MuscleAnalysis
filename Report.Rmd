---
title: "Calorie Consumption During Bicycle Work: A Statistical Analysis of an Incomplete Dataset"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: Nuno Chicoria, Boris Shilov, Murat cem Kose, Yibing Liu, Robin Vermote
output: 
  bookdown::pdf_document2:
    number_sections: true
bibliography: ./bibliography.bib
---

# Introduction

This project aimed to examine data originally gathered by @macdonald1914mechanical and conveyed to us by @greenwood1918efficiency, consisting of observations on seven people performing work using a bicycle ergometer, although our current dataset appears to include extra values and data not found in @greenwood1918efficiency, though these values may indeed be present in @macdonald1914mechanical, access to which could not be obtained in a timely manner. Hitherto it shall be assumed that every row in our dataset represents a separate individual, giving a total of 24 separate individuals across 24 rows. The dataset includes three separate measurements - weight of the individuals, calories per hour spent by individuals which serves as a measure of workout intensity, and calories spent during the task.

# Methods and procedure

```{r include=FALSE, cache=FALSE}
requirements = c("nlme", "effects", "pastecs", "lattice", 
                 "psych", "ggplot2", "GGally", "mice", "VIM", "aod", "BaM", "lme4", "ipw", "gridExtra", "cowplot")
lapply(requirements, require, character.only = T)
```

## Data exploration
First we load and examine the data.
```{r echo=FALSE}
options(digits=4)
muscledata = read.table("muscle-incomplete.txt", header=T, na.strings = "NA")
muscledata
```

And the summary:

```{r echo = FALSE}
summary(muscledata)
```

Here are some descriptive statistics.

```{r fig1, echo=FALSE, fig.cap="\\label{fig:muscleplots}Summary plots for the dataset."}
plot(muscledata)
```

Some exploratory statistics for all individuals:

```{r fig2, echo = FALSE, fig.cap="\\label{fig:muscledataboxplots}Boxplots"}
muscledata_edit = na.omit(muscledata)
attach(muscledata)
par(mfrow=c(1,3))
stat.desc(muscledata[,c("weight","calhour","calories")],basic = TRUE, desc = TRUE)
boxplot(weight, main='weight', col="pink")
boxplot(calhour, main='calhour', col="magenta")
boxplot(calories, main='calories', col="purple")
par(mfrow=c(1,1))
```

```{r echo = FALSE}
ggpairs(muscledata_edit)
```

Here we see that there is a strong positive correlation between calhour and calories (0.95). Whereas, a slightly positive correlation between weight and calories (0.11). 
Scatterplot with interaction and calories:

```{r echo = FALSE}
ggplot(muscledata_edit, aes(x=weight*calhour, y=calories))+ geom_point(colour="magenta") + geom_smooth(colour="purple", method="lm") + ggtitle("Interaction vs. calories")
```

Calculating the correlation if we exclude missing data:

```{r echo = FALSE}
cor(muscledata_edit$calhour, muscledata_edit$calories)
```

Testing the population correlationL $H_0:correlation=0$; $H1:correlation \neq 0;$ $95\%CI$.

```{r echo = FALSE}
cor.test(muscledata_edit$calhour, muscledata_edit$calories, alternative = "two.sided", method = "pearson")
```

This saying that there is no direct relation between weights and calories.

Let's try to explain heat production in function of weight and intensity of the workout, whilst allowing for interaction of the 2 predictors (whilst increasing intensity of workout, a higher weight could result in a different speed of heat production increase):
```{r echo = FALSE}
muscledata_edit = na.omit(muscledata)
muscledata.complete.case = lm(calories~weight+calhour+weight*calhour, data=muscledata_edit)
muscledata.complete.case.summary = summary(muscledata.complete.case)
muscledata.complete.case.summary
plot(allEffects(muscledata.complete.case))
```

Using the summary method, we conclude that adding weight, calhour and interaction to a model that already has the other possible components results in a significant increase in explanatory power. (note to group: was explained in last 10 slides of chapter 1, he'll probably ask about this if we don't mention it since using the anova method results in a different interpretation).

```{r echo = FALSE}
cor.test(muscledata_edit$weight, muscledata_edit$calories, alternative = "two.sided", method = "pearson")
```

## Missing data exploration

Let's explore the missingness of our data:

All missing is in calories.

```{r echo = FALSE}
aggr(muscledata, numbers = TRUE, prop = FALSE, ylab = c("Histogram of missing data", "Pattern"), col= c("pink", "purple"))
```

In second and third we see that the missing data is distributed among weights but it is biased in calhour. The missing data  is present in the lower values of calhour. We assume that this might be because of the machine that is not efficiently working with such small heat produced by the participants.This suggests MAR as a plausible missingness mechanism. Boris explain what is MAR to the client. 

```{r echo = FALSE, fig.width = 10}
par(mfrow=c(1,1))
histMiss(muscledata, col= c("pink", "purple"))
legend(40,8, legend=c("Observed Data", "Missing Data"),
       col=c("pink", "purple"), pch=19, cex=0.8)

histMiss(muscledata, pos=2, col= c("pink", "purple"))
legend(52,8, legend=c("Observed Data", "Missing Data"),
       col=c("pink", "purple"), pch=19, cex=0.8)

marginplot(muscledata[c("calhour","calories")], col= c("pink", "purple"))
legend(12, 345, legend=c("Observed Data", "Missing Data"),
       col=c("pink", "purple"), pch=19, cex=0.8)

marginplot(muscledata[c("weight","calories")], col= c("pink", "purple"))
legend(45, 345, legend=c("Observed Data", "Missing Data"),
       col=c("pink", "purple"), pch=19, cex=0.8)
```

## Complete case analysis

First we need to select the best linear model to use for CC - we can do this using stepwise AIC.

Using the stepwise method, we conclude that adding weight, calhour and interaction to a model that already has the other possible components results in a significant increase in explanatory power. (note to group: was explained in last 10 slides of chapter 1, he'll probably ask about this if we don't mention it since using the anova method reults in a different interpretation).


```{r echo = FALSE}
muscledata.stepwise = step(lm(calories ~1, data=muscledata_edit), scope=~weight+calhour+weight*calhour, direction="both")
```

Thus we deduce that the best-fitting model is:
$$
calories_i = \beta_0 + \beta_1*weight_i + \beta_2*calhour_i + \beta_3*(weight_i*calhour_i) + \epsilon_i
$$

The R summary for this model:

```{r echo = FALSE}
muscledata.complete.case = lm(calories~weight+calhour+weight*calhour, data=muscledata_edit)
summary(muscledata.complete.case)
```


Let's try to explain heat production in function of weight and intensity of the workout, whilst allowing for interaction of the 2 predictors (whilst increasing intensity of workout, a higher weight could result in a different speed of heat production increase):

This plot telling us that there is a decrease in coeficient between calhour and calories as calhour is increasing.

```{r echo = FALSE}
muscledata_edit = na.omit(muscledata)
muscledata.complete.case = lm(calories~weight+calhour+weight*calhour, data=muscledata_edit)
muscledata.complete.case.summary = summary(muscledata.complete.case)
muscledata.complete.case.summary
plot(allEffects(muscledata.complete.case),main="Complete Case Effects Plot")
```


## Multiple imputation analysis

Put in a short desc of multiple imputaton here

First we use the PMM method:

```{r echo = FALSE, include=FALSE, cache=FALSE}
muscledata.imp.pmm = mice(muscledata, meth = c("", "", "pmm"), m=100) # imputation of different values, 100 different complete datasets
muscledata.fit.pmm = with(data=muscledata.imp.pmm, exp=glm(calories~weight+calhour+weight*calhour))  # analysis, creating a Q for each imputed dataset
muscledata.pmm = pool(muscledata.fit.pmm)  # pooling the Qs together to create one estimate Q mean. if Q are approx normally distributed, we calculate mean over all Q and sum the within and between imputation variance using Rubins method
summary(muscledata.pmm)
```

```{r echo = FALSE}
summary(muscledata.pmm)
```



```{r echo = FALSE}
MI.fitted.values.pmm = complete(muscledata.imp.pmm, "long", inc=T)
muscledata.results.mi.pmm = glm(calories~weight+calhour+weight*calhour, data=MI.fitted.values.pmm)
dlist=list(calhour=seq(20,60,10))
plot(allEffects(muscledata.results.mi.pmm,xlevels=dlist)[1], main="PMM Effects Plot")
```

```{r echo = FALSE}
col <- rep(c("pink","purple")[1+as.numeric(is.na(muscledata.imp.pmm$data$calories))],101)
stripplot(calories~.imp, data=MI.fitted.values.pmm, jit=TRUE, fac=0.8, col=col, pch=20, cex=1.4, xlab="Imputation number", main="Original data vs. generated data (PMM)")
```

```{r echo = FALSE}
ggplot(MI.fitted.values.pmm, aes(x=weight*calhour, y=calories))+ geom_point(colour="red") + geom_smooth(colour="orange", method="lm") + ggtitle("Interaction vs Calories (PMM)")
```
What if we use the Bayesian norm method?

```{r echo = FALSE, include=FALSE, cache=FALSE}
muscledata.imp.norm = mice(muscledata, meth = c("", "", "norm"), m=100) # imputation of different values, 100 different complete datasets
muscledata.fit.norm = with(data=muscledata.imp.norm, exp=glm(calories~weight+calhour+weight*calhour))  # analysis, creating a Q for each imputed dataset
muscledata.norm = pool(muscledata.fit.norm)  # pooling the Qs together to create one estimate Q mean. if Q are approx normally distributed, we calculate mean over all Q and sum the within and between imputation variance using Rubins method
```

```{r echo = FALSE}
summary(muscledata.norm)
```


```{r echo = FALSE}
MI.fitted.values.norm = complete(muscledata.imp.norm, "long", inc=T)
muscledata.results.mi.norm = glm(calories~weight+calhour+weight*calhour, data=MI.fitted.values.norm)
dlist=list(calhour=seq(20,60,10))
plot(allEffects(muscledata.results.mi.norm,xlevels=dlist)[1], main="NORM effects plot")
```

```{r echo = FALSE}
col <- rep(c("pink","purple")[1+as.numeric(is.na(muscledata.imp.norm$data$calories))],101)
stripplot(calories~.imp, data=MI.fitted.values.norm, jit=TRUE, fac=0.8, col=col, pch=20, cex=1.4, xlab="Imputation number", main="Original data vs. generated data (NORM)")
```

```{r echo = FALSE}
ggplot(MI.fitted.values.norm, aes(x=weight*calhour, y=calories))+ geom_point(colour="red") + geom_smooth(colour="orange", method="lm") + ggtitle("Interaction vs Calories (NORM)")
```

## IPW analysis


```{r echo = FALSE}
IPWanal_muscledata = muscledata
IPWanal_muscledata$r = as.numeric(!is.na(IPWanal_muscledata$calories))
muscledata.ipw.glm = lm(r ~ calhour, data=IPWanal_muscledata, family=binomial)
summary(muscledata.ipw.glm)
IPWanal_muscledata$w = 1/fitted(muscledata.ipw.glm)
muscledata.results.ipw= lm(calories~weight+calhour+weight*calhour, data=IPWanal_muscledata, weights=muscledata$w)
summary(muscledata.results.ipw)
```

```{r echo = FALSE}
plot(allEffects(muscledata.results.ipw), main="IPW effects plot")
```

```{r echo = FALSE}
ggplot(MI.fitted.values.pmm, aes(x=weight*calhour, y=calories))+ geom_point(colour="red") + geom_smooth(colour="orange", method="lm") + ggtitle("Interaction vs Calories (PMM)")
```

We can take a look at the AIC values of the complete case and IPW models to compare:

```{r echo = FALSE}
AIC(muscledata.complete.case)
```
```{r echo = FALSE}
AIC(muscledata.results.ipw)
```
 


# Discussion

Due to the NA values, we conducted a full model analysis with a complete case and three NA comparsions (you can write this better) models. Beacuse the NA values are not evenly distrubited among calhour, we decided to try different approaches for NA handling.

IPW assigns weights to each observation so it uses already availible ones. Since all calorıes values ın calhour 13 are  mıssıng, the method cannot assıgn a weıght. no value can represent thıs group, other mıssıng values fall ınto calhour 19, whıle a hıgher weıght ıs assıgned to the only avaılable data ın calhour 19. so the only dıfference between cc and ıpw is only based on thıs value, thus the graph is the mostly the same for both CC and IPW and that's why we chose to represent both with the same graph.

for the MI method, we trıed two ways to generate the data correspondıng to the mıssıng data, PMM and NORM.
PMM generates the data accordıng to the pattern ın the observed ones. ın our cases, the data ıs dıscreted by the body weıght, so pmm generated the data dıscreted as well. ın norm method, the data ıs generated based on normal dıstrıbutıon.

In our MI models, there ıs no changes ın slope between each calhour and weıght ınteractıons. by checkıng the fıttıng model generated, the p-value corresponds to the weıght and the ınteractıon ıs hıgher than 0.05, whıch ındıcates these terms are not sıgnıfıcant. only the calhour effects the value of calorıes ın thıs estımated model, whıch make nonsense. ın common sense, person wıth larger body weıght generates more heats durıng the same ıntense level of workout comparıng to a person wıth lower body weıght.
İn the following three graphs we can see that the behaviour of the ınteraction factor vs. calories is simillar for the cc model and the two models created under MI. This three graphs are relevant to see how the two different methods chose in MI generate the new values.



```{r echo = FALSE}
complete=ggplot(muscledata_edit, aes(x=weight*calhour, y=calories ))+ geom_point(colour="magenta") + geom_smooth(colour="purple", method="lm") +xlab("Interaction")
norm=ggplot(MI.fitted.values.norm, aes(x=weight*calhour, y=calories))+ geom_point(colour="magenta") + geom_smooth(colour="purple", method="lm") +xlab("Interaction")
pmm=ggplot(MI.fitted.values.pmm, aes(x=weight*calhour, y=calories))+ geom_point(colour="magenta") + geom_smooth(colour="purple", method="lm") +xlab("Interaction")
plot_grid(complete, pmm, norm , 
          labels = c("CC/IPW", "PMM", "NORM"),
          ncol = 2, nrow = 2)
```


```{r echo=FALSE}
par(mfrow=c(2,1))
col <- rep(c("pink","purple")[1+as.numeric(is.na(muscledata.imp.pmm$data$calories))],101)
stripplot(calories~.imp, data=MI.fitted.values.pmm, jit=TRUE, fac=0.8, col=col, pch=20, cex=1.4, xlab="Imputation number", main="Original data vs. generated data (PMM)")
```

```{r echo=FALSE}
col <- rep(c("pink","purple")[1+as.numeric(is.na(muscledata.imp.norm$data$calories))],101)
stripplot(calories~.imp, data=MI.fitted.values.norm, jit=TRUE, fac=0.8, col=col, pch=20, cex=1.4, xlab="Imputation number", main="Original data vs. generated data (NORM)")
```


Because there are no calorie values for calhour 13, there are no data to atributte weights to. So, IPW will make a difference only for calhour 19. This gives us a slightly better model with IPW than complete case. 

```{r FigCompare, echo = FALSE, fig.width=10, fig.height=10}

CCplot = plot(allEffects(muscledata.complete.case)[[1]], multiline=TRUE, ci.style = "bands",main = "CC")
NORMplot = plot(allEffects(muscledata.results.mi.norm,xlevels=dlist)[[1]], multiline=TRUE, ci.style = "bands",main = "Norm")
PMMplot = plot(allEffects(muscledata.results.mi.pmm, xlevels=dlist)[[1]], multiline=TRUE, ci.style = "bands",main = "PMM")
IPWplot = plot(allEffects(muscledata.results.ipw)[[1]], multiline=TRUE, ci.style = "bands",main = "IPW")
class(CCplot) = class(NORMplot) = class(PMMplot) = class(IPWplot) = "trellis"
grid.arrange(CCplot, NORMplot, PMMplot, IPWplot, ncol=2, nrow=2)
```


# Conclusion

In our case, IPW doesn't come as an improvement in comparison to the CC model. 
and using standard error


The missing data is correlated with the calhour - intensity of the exercise - hence there is something wrong with the experimental design. Such as the way they measured heat production, so they could not accurately measure calorie burning. While we have no data for low calhour values, attributing weights to the values we have is not workable for the 13 calhour data point. That being said, the MI approach provides a more robust estimates for missing data.


# References

